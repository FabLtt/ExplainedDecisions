{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import metrics\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of Dataset for Novice and Expert pairs, and different sampling time \n",
    "\n",
    "        ''      stands for dt = 0.02 (default)\n",
    "\n",
    "        'step2' stands for dt = 0.04\n",
    "\n",
    "        'step4' stands for dt = 0.08\n",
    "        \n",
    "Datasets available at [ExplainedDecisions](https://osf.io/wgk8e/?view_only=8aec18499ed8457cb296032545963542) public repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_list = [\"Datasets/DatasetFileMultiClassPred_BothHerders_WrtGoal_Extended\", \n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_WrtGoal_Extended_step2\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_WrtGoal_Extended_step4\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_Expert_WrtGoal_Extended\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_Expert_WrtGoal_Extended_step2\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_Expert_WrtGoal_Extended_step4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input features (or state variables) to be extracted from the Dataset loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = ['h_t0 rel dist', 'h_t1 rel dist', 'h_t2 rel dist', 'h_t3 rel dist', 'h_t0 rel angle', 'h_t1 rel angle', 'h_t2 rel angle', 'h_t3 rel angle', \n",
    "          'h_goal rel dist', 't0_goal rel dist', 't1_goal rel dist', 't2_goal rel dist', 't3_goal rel dist',\n",
    "          'h vel_r' , 't0 vel_r' , 't1 vel_r' , 't2 vel_r' ,  't3 vel_r' , \n",
    "          'h acc_r', 't0 acc_r', 't1 acc_r', 't2 acc_r', 't3 acc_r', \n",
    "          'h_goal_th', 't0_goal_th', 't1_goal_th', 't2_goal_th', 't3_goal_th', \n",
    "          'h_dir_motion', 't0_dir_motion', 't1_dir_motion', 't2_dir_motion', 't3_dir_motion',\n",
    "          'h_h1 rel dist', 'h_h1 rel angle', 'h1_goal rel dist', 'h1 vel_r', 'h1 acc_r',\n",
    "          'h1_goal_th', 'h1_dir_motion', 'h1_t0 rel dist', 'h1_t1 rel dist', 'h1_t2 rel dist', 'h1_t3 rel dist', \n",
    "          'h1_t0 rel angle', 'h1_t1 rel angle', 'h1_t2 rel angle', 'h1_t3 rel angle','Label']\n",
    "\n",
    "Labels.insert(0,\"Herder_id\")\n",
    "Labels.insert(1,\"Trial_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of decision horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_forward_list = [1, 8, 16, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of ANN trained for each combination of sampling time ('step') and decision horizon ('look_forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './checkpoint/FinalModels/'\n",
    "\n",
    "Novice_step1 = [directory + '15032021/TrainTestSets_reduced3_indexes_001', \n",
    "                directory + '11032021/TrainTestSets_reduced3_indexes_004',\n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_001',\n",
    "                directory + '14032021/TrainTestSets_reduced3_indexes_001']\n",
    "\n",
    "Novice_step2 = [directory + '11032021/TrainTestSets_reduced3_indexes_002', \n",
    "                directory + '11032021/TrainTestSets_reduced3_indexes_005',\n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_002',\n",
    "                directory + '14032021/TrainTestSets_reduced3_indexes_002']\n",
    "\n",
    "Novice_step4 = [directory + '11032021/TrainTestSets_reduced3_indexes_003', \n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_008',\n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_003',\n",
    "                directory + '14032021/TrainTestSets_reduced3_indexes_003']\n",
    "\n",
    "Expert_step1 = [directory + '12032021/TrainTestSets_reduced3_indexes_001', \n",
    "                directory + '12032021/TrainTestSets_reduced3_indexes_004',\n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_004',\n",
    "                directory + '14032021/TrainTestSets_reduced3_indexes_004']\n",
    "\n",
    "Expert_step2 = [directory + '12032021/TrainTestSets_reduced3_indexes_002', \n",
    "                directory + '12032021/TrainTestSets_reduced3_indexes_005',\n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_005',\n",
    "                directory + '14032021/TrainTestSets_reduced3_indexes_005']\n",
    "\n",
    "Expert_step4 = [directory + '12032021/TrainTestSets_reduced3_indexes_003', \n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_007',\n",
    "                directory + '13032021/TrainTestSets_reduced3_indexes_006',\n",
    "                directory + '14032021/TrainTestSets_reduced3_indexes_006']\n",
    "\n",
    "file_set_list = [Novice_step1, Novice_step2, Novice_step4, Expert_step1, Expert_step2, Expert_step4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Novice_step1_model = [directory + '15032021/15032021001_reduced3', \n",
    "                directory + '11032021/11032021004_reduced3',\n",
    "                directory + '13032021/13032021001_reduced3',\n",
    "                directory + '14032021/14032021001_reduced3']\n",
    "\n",
    "Novice_step2_model = [directory + '11032021/11032021002_reduced3', \n",
    "                directory + '11032021/11032021005_reduced3',\n",
    "                directory + '13032021/13032021002_reduced3',\n",
    "                directory + '14032021/14032021002_reduced3']\n",
    "\n",
    "Novice_step4_model = [directory + '11032021/11032021003_reduced3', \n",
    "                directory + '13032021/13032021008_reduced3',\n",
    "                directory + '13032021/13032021003_reduced3',\n",
    "                directory + '14032021/14032021003_reduced3']\n",
    "\n",
    "Expert_step1_model = [directory + '12032021/12032021001_reduced3', \n",
    "                directory + '12032021/12032021004_reduced3',\n",
    "                directory + '13032021/13032021004_reduced3',\n",
    "                directory + '14032021/14032021004_reduced3']\n",
    "\n",
    "Expert_step2_model = [directory + '12032021/12032021002_reduced3', \n",
    "                directory + '12032021/12032021005_reduced3',\n",
    "                directory + '13032021/13032021005_reduced3',\n",
    "                directory + '14032021/14032021005_reduced3']\n",
    "\n",
    "Expert_step4_model = [directory + '12032021/12032021003_reduced3', \n",
    "                directory + '13032021/13032021007_reduced3',\n",
    "                directory + '13032021/13032021006_reduced3',\n",
    "                directory + '14032021/14032021006_reduced3']\n",
    "\n",
    "file_model_list = [Novice_step1_model, Novice_step2_model, Novice_step4_model, \n",
    "                   Expert_step1_model, Expert_step2_model, Expert_step4_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1, look_forward=1):\n",
    "    dataX, dataY, dataZ = [], [], []\n",
    "    for i in range(len(dataset)-look_back-look_forward):\n",
    "        a = dataset[i:(i+look_back), :-1]\n",
    "        b = dataset[i:(i+look_back+look_forward), -1]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back + look_forward, -1])\n",
    "        dataZ.append(b)\n",
    "    return dataX, dataY, dataZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance for each ANN trained, listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(Dataset_list)):\n",
    "    \n",
    "    # load the dataset    \n",
    "    file = open(Dataset_list[j],\"rb\")   \n",
    "    Dataset_full_df = pickle.load(file)\n",
    "    \n",
    "    # extract the input features referred by Labels\n",
    "    Dataset_df = Dataset_full_df[Labels]\n",
    "    n_features = len(Dataset_df.columns) - 3\n",
    "    print(\"\\n there are \", n_features,\" features!\")\n",
    "\n",
    "    Dataset = Dataset_df.values\n",
    "  \n",
    "    for i in range(4):\n",
    "        print(look_forward_list[i])\n",
    "        print(file_set_list[j][i])\n",
    "        \n",
    "        # Create the sequences of features and target outputs from the dataset\n",
    "        look_forward = look_forward_list[i]\n",
    "        look_back = 25\n",
    "        \n",
    "        sequences = []\n",
    "        sequences_labels = []\n",
    "        targets = []\n",
    "        \n",
    "        # A sequence can refer to only one Herder ID and one Trial ID\n",
    "        herders_tot = int(max(Dataset[:,0])) + 1\n",
    "        trial_tot = int(max(Dataset[:,1])) + 1\n",
    "                     \n",
    "        for herder_id in range(herders_tot):\n",
    "            for trial_id in range(trial_tot):\n",
    "                Dtst = Dataset_df[(Dataset_df[\"Herder_id\"]==herder_id) & (Dataset_df[\"Trial_id\"]==trial_id)].values[:,2:]\n",
    "                seq, tar, seq_lbl = create_dataset(Dtst, look_back, look_forward)\n",
    "                sequences = sequences + seq\n",
    "                targets = targets + tar\n",
    "                sequences_labels = sequences_labels + seq_lbl\n",
    "         \n",
    "        # From the total available samples select the ones used for training and test during the training phase\n",
    "        file_to_open = open(file_set_list[j][i],\"rb\")\n",
    "        indexes_data = pickle.load(file_to_open)\n",
    "        file_to_open.close()\n",
    "\n",
    "        train_index = indexes_data[0]\n",
    "        test_index = indexes_data[1]\n",
    "\n",
    "        sequences_array = np.array(sequences)\n",
    "        sequences_labels_array = np.array(sequences_labels)\n",
    "        targets_array = np.array(targets)\n",
    "\n",
    "        X_train, X_test = sequences_array[train_index], sequences_array[test_index]\n",
    "        y_train, y_test = targets_array[train_index], targets_array[test_index]\n",
    "\n",
    "        dummies_train = pd.get_dummies(y_train)\n",
    "\n",
    "        train = X_train\n",
    "        train_target = dummies_train.values    \n",
    "\n",
    "        dummies_test = pd.get_dummies(y_test)\n",
    "\n",
    "        test = X_test\n",
    "        test_target = dummies_test.values\n",
    "\n",
    "        test_set = X_test\n",
    "        test_set_target = dummies_test.values\n",
    "        \n",
    "        \n",
    "#         # Uncomment lines below to check the category samples belong to\n",
    "        \n",
    "#         Z_test = []\n",
    "#         Z_test = sequences_labels_array[test_index]\n",
    "        \n",
    "#         test_SameSeq_SameTar_indexes = []         # indexes of sequences/targets with all equal labels \n",
    "#         test_SameSeq_DiffTar_indexes = []         # indexes of sequences with equal labels but different target\n",
    "#         test_DiffSeq_SameTar_indexes = []         # indexes of sequences with different labels but 25th label equale to the target\n",
    "#         test_DiffSeq_DiffTar_indexes = []         # indexex of sequences/targets with all different labels\n",
    "\n",
    "#         for ind in range(len(Z_test)): \n",
    "#             arr = Z_test[ind]\n",
    "#             result_SameSeq_SameTar = np.all(arr == arr[0])\n",
    "#             result_SameSeq_DiffTar = np.all(arr[:-1] == arr[0]) & (arr[-1] != arr[-2])\n",
    "#             result_DiffSeq_SameTar = np.any(arr[:-1] != arr[0]) & (arr[-1] == arr[-2])\n",
    "#             result_DiffSeq_DiffTar = np.any(arr[:-1] != arr[0]) & (arr[-1] != arr[-2])\n",
    "\n",
    "#             if result_SameSeq_SameTar:\n",
    "#                 test_SameSeq_SameTar_indexes.append(ind)\n",
    "#             if result_SameSeq_DiffTar:\n",
    "#                 test_SameSeq_DiffTar_indexes.append(ind)\n",
    "#             if result_DiffSeq_SameTar:\n",
    "#                 test_DiffSeq_SameTar_indexes.append(ind)\n",
    "#             if result_DiffSeq_DiffTar:\n",
    "#                 test_DiffSeq_DiffTar_indexes.append(ind)\n",
    "                \n",
    "#         print(\"Non trasitioning and non switching sequences %i%%\" %(len(test_SameSeq_SameTar_indexes)/len(test_index)*100))\n",
    "#         print(\"Non Transitioning and switching sequences    %i%%\" %(len(test_SameSeq_DiffTar_indexes)/len(test_index)*100))\n",
    "#         print(\"Transitioning and non switching sequences    %i%%\" %(len(test_DiffSeq_SameTar_indexes)/len(test_index)*100))\n",
    "#         print(\"Transitioning and switching sequences        %i%%\" %(len(test_DiffSeq_DiffTar_indexes)/len(test_index)*100))\n",
    "#         # End of samples' category checking #\n",
    "    \n",
    "        # Load the trained ANN         \n",
    "        file_model = file_model_list[j][i]\n",
    "        \n",
    "        print(file_model)\n",
    "        \n",
    "        model = load_model(file_model)\n",
    "        \n",
    "        # Use the trained ANN on the test set        \n",
    "        test_preds = model.predict(test_set)\n",
    "        \n",
    "        # Compute metrics for the trained ANN \n",
    "        predicted_classes = np.argmax(test_preds,axis=1)\n",
    "        expected_classes = np.argmax(test_set_target,axis=1)\n",
    "        correct = metrics.accuracy_score(expected_classes,predicted_classes)\n",
    "        \n",
    "        precision_recall_f1 = metrics.precision_recall_fscore_support(expected_classes,predicted_classes)\n",
    "\n",
    "        precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "        for i in range (5): \n",
    "            precision = precision + precision_recall_f1[0][i]\n",
    "            recall = recall + precision_recall_f1[1][i]\n",
    "            f1 = f1 + precision_recall_f1[2][i]\n",
    "\n",
    "        kappascore = metrics.cohen_kappa_score(expected_classes, predicted_classes)\n",
    "\n",
    "        print(\"\\n------ Accuracy: %.2f%%\" % (correct*100))\n",
    "\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle   \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import UtilityFunctions as uf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn import metrics\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of Dataset for Novice and Expert pairs, and different sampling time \n",
    "\n",
    "        ''      stands for dt = 0.02 (default)\n",
    "\n",
    "        'step2' stands for dt = 0.04\n",
    "\n",
    "        'step4' stands for dt = 0.08\n",
    "        \n",
    "Datasets available at [ExplainedDecisions](https://osf.io/wgk8e/?view_only=8aec18499ed8457cb296032545963542) public repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_list = [\"Datasets/DatasetFileMultiClassPred_BothHerders_WrtGoal_Extended\", \n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_WrtGoal_Extended_step2\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_WrtGoal_Extended_step4\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_Expert_WrtGoal_Extended\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_Expert_WrtGoal_Extended_step2\",\n",
    "                \"Datasets/DatasetFileMultiClassPred_BothHerders_Expert_WrtGoal_Extended_step4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input features (or state variables) to be extracted from the Dataset loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = ['h_t0 rel dist', 'h_t1 rel dist', 'h_t2 rel dist', 'h_t3 rel dist', 'h_t0 rel angle', 'h_t1 rel angle', 'h_t2 rel angle', 'h_t3 rel angle', \n",
    "          'h_goal rel dist', 't0_goal rel dist', 't1_goal rel dist', 't2_goal rel dist', 't3_goal rel dist',\n",
    "          'h vel_r' , 't0 vel_r' , 't1 vel_r' , 't2 vel_r' ,  't3 vel_r' , \n",
    "          'h acc_r', 't0 acc_r', 't1 acc_r', 't2 acc_r', 't3 acc_r', \n",
    "          'h_goal_th', 't0_goal_th', 't1_goal_th', 't2_goal_th', 't3_goal_th', \n",
    "          'h_dir_motion', 't0_dir_motion', 't1_dir_motion', 't2_dir_motion', 't3_dir_motion',\n",
    "          'h_h1 rel dist', 'h_h1 rel angle', 'h1_goal rel dist', 'h1 vel_r', 'h1 acc_r',\n",
    "          'h1_goal_th', 'h1_dir_motion', 'h1_t0 rel dist', 'h1_t1 rel dist', 'h1_t2 rel dist', 'h1_t3 rel dist', \n",
    "          'h1_t0 rel angle', 'h1_t1 rel angle', 'h1_t2 rel angle', 'h1_t3 rel angle','Label']\n",
    "\n",
    "Labels.insert(0,\"Herder_id\")\n",
    "Labels.insert(1,\"Trial_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of decision horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_forward_list = [1, 8, 16, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List of ANN trained for each combination of sampling time ('step') and decision horizon ('look_forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './checkpoint/FinalModels/'\n",
    "\n",
    "Novice_step1 = [directory + '02032022/TrainTestSets_Novice_step1_thor1', \n",
    "                directory + '02032022/TrainTestSets_Novice_step1_thor8',\n",
    "                directory + '04032022/TrainTestSets_Novice_step1_thor16',\n",
    "                directory + '04032022/TrainTestSets_Novice_step1_thor32']\n",
    "\n",
    "Novice_step2 = [directory + '28022022/TrainTestSets_Novice_step2_thor1', \n",
    "                directory + '01032022/TrainTestSets_Novice_step2_thor8',\n",
    "                directory + '26022022/TrainTestSets_Novice_step2_thor16',\n",
    "                directory + '28022022/TrainTestSets_Novice_step2_thor32']\n",
    "\n",
    "Novice_step4 = [directory + '05032022/TrainTestSets_Novice_step4_thor1', \n",
    "                directory + '06032022/TrainTestSets_Novice_step4_thor8',\n",
    "                directory + '06032022/TrainTestSets_Novice_step4_thor16',\n",
    "                directory + '06032022/TrainTestSets_Novice_step4_thor32']\n",
    "\n",
    "Expert_step1 = [directory + '04032022/TrainTestSets_Expert_step1_thor1', \n",
    "                directory + '04032022/TrainTestSets_Expert_step1_thor8',\n",
    "                directory + '04032022/TrainTestSets_Expert_step1_thor16',\n",
    "                directory + '05032022/TrainTestSets_Expert_step1_thor32']\n",
    "\n",
    "Expert_step2 = [directory + '20022022/TrainTestSets_Expert_step2_thor1', \n",
    "                directory + '01032022/TrainTestSets_Expert_step2_thor8',\n",
    "                directory + '05032022/TrainTestSets_Expert_step2_thor16',\n",
    "                directory + '28022022/TrainTestSets_Expert_step2_thor32']\n",
    "\n",
    "Expert_step4 = [directory + '06032022/TrainTestSets_Expert_step4_thor1', \n",
    "                directory + '06032022/TrainTestSets_Expert_step4_thor8',\n",
    "                directory + '06032022/TrainTestSets_Expert_step4_thor16',\n",
    "                directory + '07032022/TrainTestSets_Expert_step4_thor32']\n",
    "\n",
    "file_set_list = [Novice_step1, Novice_step2, Novice_step4, Expert_step1, Expert_step2, Expert_step4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Novice_step1_model = [directory + '02032022/02032022001', \n",
    "                directory + '02032022/02032022002',\n",
    "                directory + '04032022/04032022001',\n",
    "                directory + '04032022/04032022002']\n",
    "\n",
    "Novice_step2_model = [directory + '28022022/28022022004', \n",
    "                directory + '01032022/01032022001',\n",
    "                directory + '26022022/26022022001',\n",
    "                directory + '28022022/28022022001']\n",
    "\n",
    "Novice_step4_model = [directory + '05032022/05032022003', \n",
    "                directory + '06032022/06032022001',\n",
    "                directory + '06032022/06032022002',\n",
    "                directory + '06032022/06032022003']\n",
    "\n",
    "Expert_step1_model = [directory + '04032022/04032022003', \n",
    "                directory + '04032022/04032022004',\n",
    "                directory + '04032022/04032022005',\n",
    "                directory + '05032022/05032022001']\n",
    "\n",
    "Expert_step2_model = [directory + '20022022/20022022003', \n",
    "                directory + '01032022/01032022002',\n",
    "                directory + '05032022/05032022002',\n",
    "                directory + '28022022/28022022002']\n",
    "\n",
    "Expert_step4_model = [directory + '06032022/06032022004', \n",
    "                directory + '06032022/06032022005',\n",
    "                directory + '06032022/06032022006',\n",
    "                directory + '07032022/07032022001']\n",
    "\n",
    "file_model_list = [Novice_step1_model, Novice_step2_model, Novice_step4_model, \n",
    "                   Expert_step1_model, Expert_step2_model, Expert_step4_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance for each ANN trained, listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(Dataset_list)):\n",
    "    \n",
    "    # load the dataset    \n",
    "    file = open(Dataset_list[j],\"rb\")   \n",
    "    Dataset_full_df = pickle.load(file)\n",
    "    \n",
    "    # extract the input features referred by Labels\n",
    "    Dataset_df = Dataset_full_df[Labels]\n",
    "    n_features = len(Dataset_df.columns) - 3\n",
    "    print(\"\\n there are \", n_features,\" features!\")\n",
    "\n",
    "    Dataset = Dataset_df.values\n",
    "  \n",
    "    for i in range(4):\n",
    "        print(look_forward_list[i])\n",
    "        print(file_set_list[j][i])\n",
    "        \n",
    "        # Create the sequences of features and target outputs from the dataset\n",
    "        look_forward = look_forward_list[i]\n",
    "        look_back = 25\n",
    "        \n",
    "        sequences, sequences_labels, targets = [],[],[]\n",
    "\n",
    "        herders_tot = int(max(Dataset[:,0])) + 1\n",
    "        trial_tot = int(max(Dataset[:,1])) + 1\n",
    "\n",
    "        for herder_id in range(herders_tot):\n",
    "            for trial_id in range(trial_tot):\n",
    "                Dtst = Dataset_df[(Dataset_df[\"Herder_id\"]==herder_id) & (Dataset_df[\"Trial_id\"]==trial_id)].values[:,2:]\n",
    "                seq, tar, seq_lbl = uf.create_dataset(Dtst, look_back, look_forward)\n",
    "                sequences = sequences + seq\n",
    "                targets = targets + tar\n",
    "                sequences_labels = sequences_labels + seq_lbl\n",
    "\n",
    "        sequences_array = np.array(sequences)\n",
    "        targets_array = np.array(targets)\n",
    "        sequences_labels_array = np.array(sequences_labels)\n",
    "         \n",
    "        # From the total available samples select the ones used for training and test during the training phase\n",
    "        file_to_open = open(file_set_list[j][i],\"rb\")\n",
    "        indexes_data = pickle.load(file_to_open)\n",
    "        file_to_open.close()\n",
    "\n",
    "        type_index = indexes_data[0]\n",
    "        train_index = indexes_data[1]\n",
    "        test_index = indexes_data[2]\n",
    "\n",
    "        X_senior, y_senior, Z_senior = sequences_array[type_index], targets_array[type_index], sequences_labels_array[type_index]\n",
    "        X_test = X_senior[test_index]\n",
    "        y_test = y_senior[test_index]\n",
    "        Z_test = Z_senior[test_index]\n",
    "\n",
    "        targets_labels_array = uf.checkSamplesType(Z_test)\n",
    "        dummies_train = pd.get_dummies(y_train)\n",
    "\n",
    "        train = X_train\n",
    "        train_target = dummies_train.values    \n",
    "\n",
    "        dummies_test = pd.get_dummies(y_test)\n",
    "\n",
    "        test = X_test\n",
    "        test_target = dummies_test.values\n",
    "\n",
    "        test_set = X_test\n",
    "        test_set_target = dummies_test.values\n",
    "        \n",
    "        \n",
    "        # Load the trained ANN         \n",
    "        file_model = file_model_list[j][i]\n",
    "        \n",
    "        print(file_model)\n",
    "        \n",
    "        model = load_model(file_model)\n",
    "        \n",
    "        # Use the trained ANN on the test set        \n",
    "        test_preds = model.predict(test_set)\n",
    "        \n",
    "        # Compute metrics for the trained ANN \n",
    "        predicted_classes = np.argmax(test_preds,axis=1)\n",
    "        expected_classes = np.argmax(test_set_target,axis=1)\n",
    "        correct = metrics.accuracy_score(expected_classes,predicted_classes)\n",
    "        \n",
    "        precision_recall_f1 = metrics.precision_recall_fscore_support(expected_classes,predicted_classes)\n",
    "\n",
    "        precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "        for i in range (5): \n",
    "            precision = precision + precision_recall_f1[0][i]\n",
    "            recall = recall + precision_recall_f1[1][i]\n",
    "            f1 = f1 + precision_recall_f1[2][i]\n",
    "\n",
    "        kappascore = metrics.cohen_kappa_score(expected_classes, predicted_classes)\n",
    "\n",
    "        print(\"\\n------ Accuracy: %.2f%%\" % (correct*100))\n",
    "\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle  \n",
    "from sklearn import metrics\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results have been obtained by running SHAPForPrediction_tf1.py script\n",
    "\n",
    "SHAP documentation available at [SHAP](https://github.com/slundberg/shap) GitHub repository.\n",
    "\n",
    "Datasets and SHAP results available at [ExplainedDecisions](https://osf.io/wgk8e/?view_only=8aec18499ed8457cb296032545963542) public repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check TensorFlow and SHAP version for compatibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__ , shap.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the dataset file ('file'), the training and test indexes file ('file_to_open') and the model file ('file_model')\n",
    "- Select the decision horizon Thor ('look_forward')\n",
    "- Select the sampling time ('step') linked to sequence length Tseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for the ANN trained on Novice data, in the default case of Tseq = 1s (step = 1) and a prediction horizon Thor = 1 (look_forward = 1)\n",
    "\n",
    "file = open(\"./Datasets/DatasetFileMultiClassPred_BothHerders_WrtGoal_Extended_step2\",\"rb\")\n",
    " \n",
    "look_forward = 16\n",
    "\n",
    "# step = \"\"\n",
    "\n",
    "model_id = '26022022'\n",
    "file_id = '001'\n",
    "\n",
    "directory = './checkpoint/FinalModels/'\n",
    "file_to_open = open(directory + model_id + '/TrainTestSets_Expert_step2_thor16,\"rb\")\n",
    "file_model = directory + model_id  + '/'+ model_id + file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the dataset and select the columns referred by 'Labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = ['h_t0 rel dist', 'h_t1 rel dist', 'h_t2 rel dist', 'h_t3 rel dist', 'h_t0 rel angle', 'h_t1 rel angle', \n",
    "          'h_t2 rel angle', 'h_t3 rel angle', \n",
    "          'h_goal rel dist', 't0_goal rel dist', 't1_goal rel dist', 't2_goal rel dist', 't3_goal rel dist',\n",
    "          'h vel_r' , 't0 vel_r' , 't1 vel_r' , 't2 vel_r' ,  't3 vel_r' , \n",
    "          'h acc_r', 't0 acc_r', 't1 acc_r', 't2 acc_r', 't3 acc_r', \n",
    "          'h_goal_th', 't0_goal_th', 't1_goal_th', 't2_goal_th', 't3_goal_th', \n",
    "          'h_dir_motion', 't0_dir_motion', 't1_dir_motion', 't2_dir_motion', 't3_dir_motion',\n",
    "          'h_h1 rel dist', 'h_h1 rel angle', 'h1_goal rel dist', 'h1 vel_r', 'h1 acc_r',\n",
    "          'h1_goal_th', 'h1_dir_motion', 'h1_t0 rel dist', 'h1_t1 rel dist', 'h1_t2 rel dist', 'h1_t3 rel dist', \n",
    "          'h1_t0 rel angle', 'h1_t1 rel angle', 'h1_t2 rel angle', 'h1_t3 rel angle','Label']\n",
    "\n",
    "Labels.insert(0,\"Herder_id\")\n",
    "Labels.insert(1,\"Trial_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_full_df = pickle.load(file)\n",
    "file.close()\n",
    "    \n",
    "Dataset_df = Dataset_full_df[Labels]\n",
    "\n",
    "n_features = len(Dataset_df.columns) - 3\n",
    "print(\"there are \", n_features,\" features!\")\n",
    "\n",
    "Dataset = Dataset_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the sequences of features and target outputs from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, sequences_labels, targets = [],[],[]\n",
    "\n",
    "herders_tot = int(max(Dataset[:,0])) + 1\n",
    "trial_tot = int(max(Dataset[:,1])) + 1\n",
    "\n",
    "for herder_id in range(herders_tot):\n",
    "    for trial_id in range(trial_tot):\n",
    "        Dtst = Dataset_df[(Dataset_df[\"Herder_id\"]==herder_id) & (Dataset_df[\"Trial_id\"]==trial_id)].values[:,2:]\n",
    "        seq, tar, seq_lbl = uf.create_dataset(Dtst, look_back, look_forward)\n",
    "        sequences = sequences + seq\n",
    "        targets = targets + tar\n",
    "        sequences_labels = sequences_labels + seq_lbl\n",
    "\n",
    "sequences_array = np.array(sequences)\n",
    "targets_array = np.array(targets)\n",
    "sequences_labels_array = np.array(sequences_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select from the total available samples the ones used for training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_index = indexes_data[0]\n",
    "train_index = indexes_data[1]\n",
    "test_index = indexes_data[2]\n",
    "\n",
    "X_senior, y_senior, Z_senior = sequences_array[type_index], targets_array[type_index], sequences_labels_array[type_index]\n",
    "X_test = X_senior[test_index]\n",
    "y_test = y_senior[test_index]\n",
    "Z_test = Z_senior[test_index]\n",
    "        \n",
    "dummies_test = pd.get_dummies(y_test)\n",
    "\n",
    "targets_labels_array = uf.checkSamplesType(Z_test)\n",
    "dummies_train = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute performance metrics of the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = X_test\n",
    "# test_set_target = dummies_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the trained ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(file_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the trained ANN on the test set and Compute metrics for the trained ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "\n",
    "predicted_classes = np.argmax(test_preds,axis=1)\n",
    "expected_classes = np.argmax(dummies_test.values,axis=1)\n",
    "correct = metrics.accuracy_score(expected_classes,predicted_classes)\n",
    "\n",
    "print(\"------ Accuracy: %.2f%%\" % (correct*100))\n",
    "\n",
    "precision_recall_f1 = metrics.precision_recall_fscore_support(expected_classes,predicted_classes)\n",
    "\n",
    "precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "for i in range (5): \n",
    "    precision = precision + precision_recall_f1[0][i]\n",
    "    recall = recall + precision_recall_f1[1][i]\n",
    "    f1 = f1 + precision_recall_f1[2][i]\n",
    "\n",
    "print(\"Macro-Precision: %.2f%%\" % (precision*100 / 5))\n",
    "print(\"-- Macro-Recall: %.2f%%\" % (recall*100 / 5))\n",
    "print(\"------ Macro-F1: %.2f%%\" % (f1*100 /5))\n",
    "\n",
    "kappascore = metrics.cohen_kappa_score(expected_classes, predicted_classes)\n",
    "print(\"---- KappaScore: %.2f%%\" % (kappascore*100))\n",
    "\n",
    "confusionMatrix = metrics.confusion_matrix(expected_classes, predicted_classes, normalize='true')\n",
    "metrics.ConfusionMatrixDisplay(confusionMatrix).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and print the SHAP values' file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_shap = file_model+'_ShapVal'\n",
    "\n",
    "with open(file_name_shap,'rb') as file:\n",
    "    shap_values = pickle.load(file)\n",
    "    \n",
    "shap_values = shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.initjs()   # uncomment this line to display SHAP plots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each class, compute the mean of shap values associated to each input features and display the top 10 [https://github.com/slundberg/shap/issues/632]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list = []\n",
    "\n",
    "for class_id in range(5):\n",
    "    vals = np.abs(shap_values[class_id]).mean(0)\n",
    "    feature_importance = pd.DataFrame(list(zip(Labels[2:-1], sum(vals))), columns=['var','feature_importance_vals_class'+str(class_id)])\n",
    "    feature_importance.sort_values(by=['feature_importance_vals_class'+str(class_id)], ascending=False,inplace=True)\n",
    "    feat_list.append(feature_importance.values[:10,0])\n",
    "    print('\\n class ', class_id)\n",
    "    print(feature_importance.values[:10,0],\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
